# ============================================================================
# EDGE-RAG UNIFIED CONFIGURATION
# ============================================================================
#
# Version: 3.1.0 - UNIFIED
# Author: Edge-RAG Research Project
# Last Modified: 2026-01-25
#
# CONSOLIDATION: settings.yaml + settings_kuzu.yaml → UNIFIED settings.yaml
#
# This configuration supports both KuzuDB and NetworkX graph backends.
# Select your preferred backend in the 'graph' section below.
#
# ============================================================================

# ============================================================================
# EMBEDDING MODEL
# ============================================================================
embeddings:
  # Model name (must match Ollama installed model)
  model_name: "nomic-embed-text"
  
  # Ollama API endpoint
  base_url: "http://localhost:11434"
  
  # Embedding dimensionality
  # nomic-embed-text: 768
  # all-MiniLM-L6-v2: 384
  # bge-base-en: 768
  embedding_dim: 768

# ============================================================================
# DOCUMENT CHUNKING
# ============================================================================
chunking:
  # Mode: "standard" (fixed size) or "semantic" (structure-aware)
  mode: "standard"
  
  # Standard chunking parameters
  chunk_size: 512         # Characters per chunk
  chunk_overlap: 128      # Overlap between chunks
  
  # Semantic chunking parameters (if mode="semantic")
  semantic:
    min_chunk_size: 200
    max_chunk_size: 1000
    detect_structure: true
    quality_filter: true
  
  # Content to filter out
  filter_patterns:
    - "^\\s*$"            # Empty lines
    - "^Page \\d+$"       # Page numbers
    - "^\\d+$"            # Standalone numbers

# ============================================================================
# VECTOR STORE (LanceDB)
# ============================================================================
vector_store:
  provider: "lancedb"
  db_path: "./data/vector_db"
  
  # Index type: "flat" (exact) or "ivfflat" (approximate, faster)
  index_type: "ivfflat"
  
  # =========================================================================
  # DISTANCE METRIC - CRITICAL SETTING
  # =========================================================================
  # Must be "cosine" for text embeddings.
  # LanceDB defaults to L2 which gives incorrect similarity scores!
  distance_metric: "cosine"
  
  # L2 normalization for numerical stability
  normalize_embeddings: true
  
  # Retrieval parameters
  top_k_vectors: 10
  similarity_threshold: 0.3    # Minimum similarity [0.0, 1.0]

# ============================================================================
# KNOWLEDGE GRAPH
# ============================================================================
graph:
  enabled: true
  
  # =========================================================================
  # BACKEND SELECTION
  # =========================================================================
  # Options:
  #   "kuzu"     - KuzuDB (recommended): Native Cypher, 10-100x faster
  #   "networkx" - NetworkX (fallback): Pure Python, GraphML persistence
  #
  # KuzuDB requires: pip install kuzu
  # NetworkX requires: pip install networkx
  #
  # If selected backend is unavailable, system will auto-fallback to other.
  # =========================================================================
  backend: "kuzu"
  
  # Database path
  # KuzuDB: Directory (e.g., "./data/knowledge_graph")
  # NetworkX: File (e.g., "./data/knowledge_graph/graph.graphml")
  graph_path: "./data/knowledge_graph"
  
  # Graph traversal settings
  max_hops: 2                    # Maximum path length for traversal
  top_k_entities: 5              # Number of graph results
  expand_context: true           # Include surrounding chunks (KuzuDB only)
  
  # Entity extraction method
  # "keyword" - Simple keyword-based (fast, basic)
  # "spacy"   - spaCy NER (requires: pip install spacy)
  # "gliner"  - GLiNER zero-shot NER (requires: pip install gliner)
  entity_extraction_method: "keyword"
  
  # Relationship types stored in graph
  relation_types:
    - "from_source"     # Chunk -> Document
    - "next_chunk"      # Chunk -> Chunk (sequential)
    - "mentions"        # Chunk -> Entity
    - "related_to"      # Entity -> Entity

# ============================================================================
# RETRIEVAL AUGMENTED GENERATION (RAG)
# ============================================================================
rag:
  # =========================================================================
  # RETRIEVAL MODE
  # =========================================================================
  # Options:
  #   "vector" - Vector similarity only (baseline)
  #   "graph"  - Graph traversal only (structural)
  #   "hybrid" - Weighted combination (recommended)
  # =========================================================================
  retrieval_mode: "hybrid"
  
  # =========================================================================
  # HYBRID WEIGHTS - Ablation Study Configuration
  # =========================================================================
  # Experiment 1: Vector-only  → vector_weight=1.0, graph_weight=0.0
  # Experiment 2: Graph-only   → vector_weight=0.0, graph_weight=1.0
  # Experiment 3: Hybrid 70/30 → vector_weight=0.7, graph_weight=0.3
  # Experiment 4: Hybrid 50/50 → vector_weight=0.5, graph_weight=0.5
  # Experiment 5: Graph-heavy  → vector_weight=0.3, graph_weight=0.7
  # =========================================================================
  vector_weight: 0.7
  graph_weight: 0.3
  
  # Future work options
  reranking_enabled: false
  query_expansion_enabled: false

# ============================================================================
# LLM CONFIGURATION (for Artifact B - Logic Layer)
# ============================================================================
llm:
  # Model name (Ollama)
  model_name: "phi3"
  
  # Alternative models (ordered by speed):
  # - "phi3:mini"      # Fastest, ~3.8B params
  # - "phi3"           # Standard, ~3.8B params
  # - "qwen2:1.5b"     # Very fast, good quality
  # - "llama3.1:8b"    # Larger, better quality
  # - "mistral"        # 7B, good balance
  
  base_url: "http://localhost:11434"
  temperature: 0.1    # Low for factual responses
  max_tokens: 300     # Response length limit

# ============================================================================
# AGENTIC CONTROLLER (Artifact B)
# ============================================================================
agent:
  # Maximum verification iterations in self-correction loop
  max_verification_iterations: 3
  
  # Enable/disable verification stage
  enable_verification: true

# ============================================================================
# QUANTIZATION SETTINGS
# ============================================================================
quantization:
  enabled: true
  bits: 4           # 4-bit quantization (llama.cpp default)
  group_size: 128   # Group size for quantization

# ============================================================================
# DATA PATHS
# ============================================================================
paths:
  root: "./"
  data: "./data"
  documents: "./data/documents"
  vector_db: "./data/vector_db"
  graph_db: "./data/knowledge_graph"
  logs: "./logs"
  cache: "./cache"

# ============================================================================
# LOGGING
# ============================================================================
logging:
  level: "INFO"      # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/edge_rag.log"

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================
performance:
  # Embedding batch size (higher = faster ingestion, more memory)
  batch_size: 64
  
  # Number of worker threads for parallel processing
  num_workers: 2
  
  # Compute device: "cpu" or "cuda" (if available)
  device: "cpu"
  
  # Enable embedding cache (SQLite-based)
  cache_embeddings: true
  
  # Maximum cache size in MB
  max_cache_size_mb: 512

# ============================================================================
# BENCHMARK CONFIGURATION (for Thesis Evaluation)
# ============================================================================
benchmark:
  # Available datasets for evaluation
  datasets:
    - "hotpotqa"
    - "2wikimultihop"
    - "strategyqa"
  
  # Default number of samples per dataset
  default_samples: 500
  
  # Ablation study configurations
  ablation_configs:
    - name: "vector_only"
      vector_weight: 1.0
      graph_weight: 0.0
    - name: "graph_only"
      vector_weight: 0.0
      graph_weight: 1.0
    - name: "hybrid_70_30"
      vector_weight: 0.7
      graph_weight: 0.3
    - name: "hybrid_50_50"
      vector_weight: 0.5
      graph_weight: 0.5

      # ============================================================================
# INGESTION CONFIGURATION
# ============================================================================
# 
# Dieses Template zeigt alle verfügbaren Chunking-Optionen.
# Kopiere die relevanten Teile in deine config/settings.yaml
#
# ============================================================================

# Füge diesen Block zu deiner settings.yaml hinzu:
ingestion:
  # ──────────────────────────────────────────────────────────────────────────
  # CHUNKING STRATEGY
  # ──────────────────────────────────────────────────────────────────────────
  # Verfügbare Strategien:
  #   - sentence:  Gruppiert N Sätze pro Chunk (schnell, einfach)
  #   - semantic:  Semantische Grenzen mit Quality Filtering (beste Qualität)
  #   - fixed:     Feste Zeichenanzahl mit Overlap
  #   - recursive: LangChain RecursiveCharacterTextSplitter
  
  chunking_strategy: "sentence"  # Empfohlen für Benchmark: "sentence"
  
  # ──────────────────────────────────────────────────────────────────────────
  # CHUNK SIZE SETTINGS
  # ──────────────────────────────────────────────────────────────────────────
  chunk_size: 1024          # Max Zeichen pro Chunk (für fixed/recursive/semantic)
  chunk_overlap: 128        # Überlappung zwischen Chunks
  min_chunk_size: 50        # Chunks kleiner als dies werden gefiltert
  
  # ──────────────────────────────────────────────────────────────────────────
  # SENTENCE STRATEGY SETTINGS
  # ──────────────────────────────────────────────────────────────────────────
  sentences_per_chunk: 3    # Anzahl Sätze pro Chunk
  
  # ──────────────────────────────────────────────────────────────────────────
  # SEMANTIC STRATEGY SETTINGS (nur wenn chunking_strategy: "semantic")
  # ──────────────────────────────────────────────────────────────────────────
  min_lexical_diversity: 0.3      # TTR threshold (0.0-1.0)
  min_information_density: 2.0    # Shannon Entropy threshold
  
  # ──────────────────────────────────────────────────────────────────────────
  # QUALITY FILTERING
  # ──────────────────────────────────────────────────────────────────────────
  filter_short_chunks: true       # Kurze Chunks entfernen
  filter_low_quality: false       # Low-quality Chunks entfernen (nur semantic)
  
  # ──────────────────────────────────────────────────────────────────────────
  # METADATA & ENTITY EXTRACTION
  # ──────────────────────────────────────────────────────────────────────────
  extract_entities: true          # Named Entities für Knowledge Graph
  add_source_metadata: true       # Quellinformationen hinzufügen


# ============================================================================
# BEISPIEL-KONFIGURATIONEN FÜR VERSCHIEDENE USE CASES
# ============================================================================

# --- BENCHMARK (Schnell, reproduzierbar) ---
# ingestion:
#   chunking_strategy: "sentence"
#   sentences_per_chunk: 3
#   min_chunk_size: 50
#   extract_entities: true

# --- PRODUCTION (Beste Qualität) ---
# ingestion:
#   chunking_strategy: "semantic"
#   chunk_size: 1024
#   chunk_overlap: 128
#   min_chunk_size: 200
#   min_lexical_diversity: 0.3
#   min_information_density: 2.0
#   filter_low_quality: true
#   extract_entities: true

# --- LANGE DOKUMENTE (PDFs, Papers) ---
# ingestion:
#   chunking_strategy: "recursive"
#   chunk_size: 2048
#   chunk_overlap: 256
#   min_chunk_size: 100
#   extract_entities: true

# --- ABLATION STUDY (Verschiedene Chunk-Größen testen) ---
# Für Ablation: Erstelle mehrere Config-Dateien:
#   - config/chunk_small.yaml  (sentences_per_chunk: 2)
#   - config/chunk_medium.yaml (sentences_per_chunk: 3)
#   - config/chunk_large.yaml  (sentences_per_chunk: 5)