# ============================================================================
# EDGE-RAG UNIFIED CONFIGURATION
# ============================================================================
#
# Version: 3.1.0 - UNIFIED
# Author: Edge-RAG Research Project
# Last Modified: 2026-01-25
#
# CONSOLIDATION: settings.yaml + settings_kuzu.yaml → UNIFIED settings.yaml
#
# This configuration supports both KuzuDB and NetworkX graph backends.
# Select your preferred backend in the 'graph' section below.
#
# ============================================================================

# ============================================================================
# EMBEDDING MODEL
# ============================================================================
embeddings:
  # Model name (must match Ollama installed model)
  model_name: "nomic-embed-text"
  
  # Ollama API endpoint
  base_url: "http://localhost:11434"
  
  # Embedding dimensionality
  # nomic-embed-text: 768
  # all-MiniLM-L6-v2: 384
  # bge-base-en: 768
  embedding_dim: 768

# ============================================================================
# DOCUMENT CHUNKING
# ============================================================================
chunking:
  # Mode: "standard" (fixed size) or "semantic" (structure-aware)
  mode: "standard"
  
  # Standard chunking parameters
  chunk_size: 512         # Characters per chunk
  chunk_overlap: 128      # Overlap between chunks
  
  # Semantic chunking parameters (if mode="semantic")
  semantic:
    min_chunk_size: 200
    max_chunk_size: 1000
    detect_structure: true
    quality_filter: true
  
  # Content to filter out
  filter_patterns:
    - "^\\s*$"            # Empty lines
    - "^Page \\d+$"       # Page numbers
    - "^\\d+$"            # Standalone numbers

# ============================================================================
# VECTOR STORE (LanceDB)
# ============================================================================
vector_store:
  provider: "lancedb"
  db_path: "./data/vector_db"
  
  # Index type: "flat" (exact) or "ivfflat" (approximate, faster)
  index_type: "ivfflat"
  
  # =========================================================================
  # DISTANCE METRIC - CRITICAL SETTING
  # =========================================================================
  # Must be "cosine" for text embeddings.
  # LanceDB defaults to L2 which gives incorrect similarity scores!
  distance_metric: "cosine"
  
  # L2 normalization for numerical stability
  normalize_embeddings: true
  
  # Retrieval parameters
  top_k_vectors: 10
  similarity_threshold: 0.3    # Minimum similarity [0.0, 1.0]

# ============================================================================
# KNOWLEDGE GRAPH
# ============================================================================
graph:
  enabled: true
  
  # =========================================================================
  # BACKEND SELECTION
  # =========================================================================
  # Options:
  #   "kuzu"     - KuzuDB (recommended): Native Cypher, 10-100x faster
  #   "networkx" - NetworkX (fallback): Pure Python, GraphML persistence
  #
  # KuzuDB requires: pip install kuzu
  # NetworkX requires: pip install networkx
  #
  # If selected backend is unavailable, system will auto-fallback to other.
  # =========================================================================
  backend: "kuzu"
  
  # Database path
  # KuzuDB: Directory (e.g., "./data/knowledge_graph")
  # NetworkX: File (e.g., "./data/knowledge_graph/graph.graphml")
  graph_path: "./data/knowledge_graph"
  
  # Graph traversal settings
  max_hops: 2                    # Maximum path length for traversal
  top_k_entities: 5              # Number of graph results
  expand_context: true           # Include surrounding chunks (KuzuDB only)
  
  # Entity extraction method
  # "keyword" - Simple keyword-based (fast, basic)
  # "spacy"   - spaCy NER (requires: pip install spacy)
  # "gliner"  - GLiNER zero-shot NER (requires: pip install gliner)
  entity_extraction_method: "keyword"
  
  # Relationship types stored in graph
  relation_types:
    - "from_source"     # Chunk -> Document
    - "next_chunk"      # Chunk -> Chunk (sequential)
    - "mentions"        # Chunk -> Entity
    - "related_to"      # Entity -> Entity

# ============================================================================
# RETRIEVAL AUGMENTED GENERATION (RAG)
# ============================================================================
rag:
  # =========================================================================
  # RETRIEVAL MODE
  # =========================================================================
  # Options:
  #   "vector" - Vector similarity only (baseline)
  #   "graph"  - Graph traversal only (structural)
  #   "hybrid" - Weighted combination (recommended)
  # =========================================================================
  retrieval_mode: "hybrid"
  
  # =========================================================================
  # HYBRID WEIGHTS - Ablation Study Configuration
  # =========================================================================
  # Experiment 1: Vector-only  → vector_weight=1.0, graph_weight=0.0
  # Experiment 2: Graph-only   → vector_weight=0.0, graph_weight=1.0
  # Experiment 3: Hybrid 70/30 → vector_weight=0.7, graph_weight=0.3
  # Experiment 4: Hybrid 50/50 → vector_weight=0.5, graph_weight=0.5
  # Experiment 5: Graph-heavy  → vector_weight=0.3, graph_weight=0.7
  # =========================================================================
  vector_weight: 0.7
  graph_weight: 0.3
  
  # Future work options
  reranking_enabled: false
  query_expansion_enabled: false

# ============================================================================
# LLM CONFIGURATION (for Artifact B - Logic Layer)
# ============================================================================
llm:
  # Model name (Ollama)
  model_name: "phi3"
  
  # Alternative models (ordered by speed):
  # - "phi3:mini"      # Fastest, ~3.8B params
  # - "phi3"           # Standard, ~3.8B params
  # - "qwen2:1.5b"     # Very fast, good quality
  # - "llama3.1:8b"    # Larger, better quality
  # - "mistral"        # 7B, good balance
  
  base_url: "http://localhost:11434"
  temperature: 0.1    # Low for factual responses
  max_tokens: 300     # Response length limit

# ============================================================================
# AGENTIC CONTROLLER (Artifact B)
# ============================================================================
agent:
  # Maximum verification iterations in self-correction loop
  max_verification_iterations: 3
  
  # Enable/disable verification stage
  enable_verification: true

# ============================================================================
# QUANTIZATION SETTINGS
# ============================================================================
quantization:
  enabled: true
  bits: 4           # 4-bit quantization (llama.cpp default)
  group_size: 128   # Group size for quantization

# ============================================================================
# DATA PATHS
# ============================================================================
paths:
  root: "./"
  data: "./data"
  documents: "./data/documents"
  vector_db: "./data/vector_db"
  graph_db: "./data/knowledge_graph"
  logs: "./logs"
  cache: "./cache"

# ============================================================================
# LOGGING
# ============================================================================
logging:
  level: "INFO"      # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/edge_rag.log"

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================
performance:
  # Embedding batch size (higher = faster ingestion, more memory)
  batch_size: 64
  
  # Number of worker threads for parallel processing
  num_workers: 2
  
  # Compute device: "cpu" or "cuda" (if available)
  device: "cpu"
  
  # Enable embedding cache (SQLite-based)
  cache_embeddings: true
  
  # Maximum cache size in MB
  max_cache_size_mb: 512

# ============================================================================
# BENCHMARK CONFIGURATION (for Thesis Evaluation)
# ============================================================================
benchmark:
  # Available datasets for evaluation
  datasets:
    - "hotpotqa"
    - "2wikimultihop"
    - "strategyqa"
  
  # Default number of samples per dataset
  default_samples: 500
  
  # Ablation study configurations
  ablation_configs:
    - name: "vector_only"
      vector_weight: 1.0
      graph_weight: 0.0
    - name: "graph_only"
      vector_weight: 0.0
      graph_weight: 1.0
    - name: "hybrid_70_30"
      vector_weight: 0.7
      graph_weight: 0.3
    - name: "hybrid_50_50"
      vector_weight: 0.5
      graph_weight: 0.5